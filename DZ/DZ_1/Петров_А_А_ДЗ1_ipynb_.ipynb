{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sJbxumiS6aQl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P9bWaC9QQJm"
      },
      "source": [
        "## **Задача 2**. Cделать нейрон, соответствующий оператору НЕ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m7w8SIPbTpI0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn # модуль с базовыми слоями\n",
        "import torch.nn.functional as F # модуль с базовыми функциями\n",
        "class Neuron(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc=nn.Linear(in_features=1,out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.heaviside(self.fc(x), torch.tensor([0.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVGdnHdZTpaJ",
        "outputId": "b7ebbd70-b746-4501-8200-c7661867558d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[0.4849]], requires_grad=True), Parameter containing:\n",
              " tensor([0.4113], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "neuron = Neuron()\n",
        "neuron.fc.weight, neuron.fc.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L1Yz2NQ5TuFd"
      },
      "outputs": [],
      "source": [
        "neuron.fc.weight.data = torch.tensor([[-1.0]])\n",
        "neuron.fc.bias.data = torch.tensor([1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdWGbnggUGNK",
        "outputId": "0a13c198-f2ef-43f7-bbc7-a24ba3ea274c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], grad_fn=<NotImplemented>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x = torch.tensor([1.0])\n",
        "neuron(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRxJxcRJQsMz"
      },
      "source": [
        "## **Задача 3**. Cделать нейрон, соответствующий оператору И."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6GghChKfUW-9"
      },
      "outputs": [],
      "source": [
        "class Neuron(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.heaviside(self.fc(x), torch.tensor([0.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ORoJk2FUXZ3",
        "outputId": "5e439aba-6905-4e42-d218-4f10e22ab029"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[-0.3508, -0.1884]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.4855], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "neuron = Neuron()\n",
        "neuron.fc.weight, neuron.fc.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uckDlmWXUYsn"
      },
      "outputs": [],
      "source": [
        "neuron.fc.weight.data = torch.tensor([[1.0, 1.0]])\n",
        "neuron.fc.bias.data = torch.tensor([-1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-2KGnx0Uaj2",
        "outputId": "1e4fe952-b6c1-4e30-d6b2-d43bf4e30811"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], grad_fn=<NotImplemented>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 1.0])\n",
        "neuron(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRuSrP7JQ00i"
      },
      "source": [
        "## **Задача 4**. Cделать нейрон, соответствующий оператору ИЛИ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M34EZ9UZUmEB"
      },
      "outputs": [],
      "source": [
        "neuron.fc.weight.data = torch.tensor([[1.0, 1.0]])\n",
        "neuron.fc.bias.data = torch.tensor([-0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nzQEVNWUmbq",
        "outputId": "fc4e3de5-9344-43fa-a864-5a22b7956f1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], grad_fn=<NotImplemented>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x = torch.tensor([0.0,1.0])\n",
        "neuron(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiPV2u5zfwyo"
      },
      "source": [
        "## **Вопрос 1**. Какие нейронные сети могут иметь только линейную разделяющую поверхность?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKGSO4obU3YX"
      },
      "source": [
        "Такие, в которых Только один нейрон, либо такие, которые НЕ имеют функцию активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEf1aXsdgLBh"
      },
      "source": [
        "## **Вопрос 2**. Имеет ли смысл соединять полносвязанные нейроны (нейроны, которые принимают на вход все выходы предыдущего слоя) с линейной функцией активации в многослойную нейронную сеть?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fojoLik8VP-P"
      },
      "source": [
        "Нет смысла"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6riR-YEuYg"
      },
      "source": [
        "##**Домашнее задание 1:** реализуйте XOR с помощью 3 нейронов. Запишите ответ в виде выражения, состоящего из объектов neuron() – моделей нейрона с пороговой функцией активации, внутри скобок может быть что угодно. Входы верхнего уровня называются x1 и x2. Пример фрагмента записи: neuron(1*x1 + 5*x2 - 0.1) + neuron(x1) (ответ будет выглядеть чуть сложнее, но других символов вроде && не потребуется)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEnnCyCV2yl"
      },
      "source": [
        "neuron(neuron(1 *x1-1*x2)+neuron(-1*x1+1*x2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0fcXB_FKV8cp"
      },
      "outputs": [],
      "source": [
        "class Neuron(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.heaviside(self.fc(x), torch.tensor([0.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q85rHiN9YxC-",
        "outputId": "18196568-4796-4826-ce5a-f8a6534acef6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[1., 1.]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.5000], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "neuron1 = Neuron()\n",
        "neuron2= Neuron()\n",
        "neuronOR=Neuron()\n",
        "neuron.fc.weight, neuron.fc.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ETBKS8RwY4sJ"
      },
      "outputs": [],
      "source": [
        "neuron1.fc.weight.data = torch.tensor([[1.0, -1.0]])\n",
        "neuron1.fc.bias.data = torch.tensor([0.0])\n",
        "\n",
        "neuron2.fc.weight.data = torch.tensor([[-1.0, 1.0]])\n",
        "neuron2.fc.bias.data = torch.tensor([0.0])\n",
        "\n",
        "neuronOR.fc.weight.data = torch.tensor([[1.0, 1.0]])\n",
        "neuronOR.fc.bias.data = torch.tensor([-0.4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAsE8R6xZYzA",
        "outputId": "38b390b2-446f-4be1-80e7-01438f6f9c2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], grad_fn=<NotImplemented>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "x = torch.tensor([0.0,1.0])# здесь менять\n",
        "a= torch.Tensor([neuron1(x).data , neuron2(x).data])\n",
        "neuronOR(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxKEOwWDwv3Y"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Выбран кодовый формат\n",
        "```\n",
        "\n",
        "##**Домашнее задание 3:** Поэксперементируйте с размером тензоров, которые влезут на видеоркарту в Colab. Найдите максимальный размер тензора для типа данных float32, float64, float16, int32, int64. На сколько они отличаются."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U-x_mo-ai1bG"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.get_device_properties(0)\n",
        "def allocate_empty_tensor(dim_size):\n",
        "  a=torch.zeros(4096,dim_size,dtype=torch.float32,device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zpnSYfd8kAHu"
      },
      "outputs": [],
      "source": [
        "allocate_empty_tensor(927600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SYUEEBvWkD9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d8171d-0ca3-4836-8f71-593c162639ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 15:53:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W /  70W |  15102MiB / 15109MiB |      4%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWuzet-khwp"
      },
      "source": [
        " Максимальный размер для torch.float32 - 4096 на 927600 элементов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHiHU0xFjLti",
        "outputId": "4152b1f8-fe42-4f12-dfcd-6746a19c2d10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_reserved()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8EonJH_XksC1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.get_device_properties(0)\n",
        "def allocate_empty_tensor(dim_size):\n",
        "  a=torch.zeros(4096,dim_size,dtype=torch.float64,device='cuda')\n",
        "  print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j0Lq_IkFk05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4599508-6fec-4ab7-b801-ab533a38bb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4096, 463900])\n"
          ]
        }
      ],
      "source": [
        "allocate_empty_tensor(463900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gVKPgHLYk2fZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb1de32-aad6-4221-d583-85fc15ed9eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 15:53:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    50W /  70W |  15106MiB / 15109MiB |    100%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n71-b3vDlLqz"
      },
      "source": [
        "Максимальный размер для float 64 - 4096 на 463900 элементов "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP4j_U3xmNLt"
      },
      "source": [
        "Линейная зависимотсть между float32 и float64. Можно предположить, что float16 вмещает в 4 раза больше элементов, чем float 64, то есть float 16 = 4096 на 1 \n",
        " 855 600 элементов\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_reserved()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGVil40rF_1",
        "outputId": "fa80ba87-9f92-4fe3-8fdf-2538b14d5342"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8ywloGVCmWh9"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.get_device_properties(0)\n",
        "def allocate_empty_tensor(dim_size):\n",
        "  a=torch.zeros(4096,dim_size,dtype=torch.int64,device='cuda')\n",
        "  print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "z2jQTmj5moK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c794eb5-410a-4632-fe22-3ba116933b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4096, 463900])\n"
          ]
        }
      ],
      "source": [
        "allocate_empty_tensor(463900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lI4TMzdSmphi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5603ae7c-ba77-4ced-f513-460d0a4de3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 15:54:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W /  70W |  15106MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_reserved()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek8wFNDvrKkC",
        "outputId": "f1c31a7f-566b-436f-a3be-defe16b00b20"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPEZMfFDmvve"
      },
      "source": [
        "Такая же ситуация и с INT. Следовательно тип int64=float64, int32=float32 по числу элементов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBR5pWOlzOtl"
      },
      "source": [
        "##**Домашнее задание 4:** Напишите хороший пример неэффективного кода для занятия памяти видеокарты, который вызовет ошибку out of memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HTBxOCm5oaCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7a10dbc4-d8eb-4301-edf2-583426b8d685"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-59843a4217c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB (GPU 0; 14.76 GiB total capacity; 10.06 GiB already allocated; 3.90 GiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "a=torch.rand([40000,1],dtype=torch.float32,device='cuda')\n",
        "b=torch.ones([1,40000],dtype=torch.float32,device='cuda')\n",
        "c=a*b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_reserved()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94vb4OeMrwMB",
        "outputId": "aed189a2-1ef0-4daf-878e-592edf814e70"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Домашнее задание 5:** Используя один линейный слой `nn.Linear` и один входной тензор `x` подберите подберите размерности так, чтобы занимать всю видеопамять.\n",
        "Попробуйте применить линейный слой к тензору `x`. Что произойдет? Кратко опишите ваши эксперименты. Что вы поняли?"
      ],
      "metadata": {
        "id": "0_hOtvvR89jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.heaviside(self.fc(x), torch.tensor([0.0],dtype=torch.float32,device='cuda'))"
      ],
      "metadata": {
        "id": "5HStQnICrxDZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = Neuron()\n",
        "neuron.fc.weight, neuron.fc.bias\n",
        "neuron.fc.weight.data = torch.rand([30000,30000],dtype=torch.float32,device='cuda')\n",
        "neuron.fc.bias.data = torch.rand([30000,30000],dtype=torch.float32,device='cuda')"
      ],
      "metadata": {
        "id": "wamKBCQlsP9n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand([30000, 30000],dtype=torch.float32,device='cuda')"
      ],
      "metadata": {
        "id": "LOFBEkIqscdd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1iPO-fsi79",
        "outputId": "7e6ec18d-c263-4075-9267-408d3fc74bb5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 15:54:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    26W /  70W |  10910MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neuron.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB_GCQ54soE8",
        "outputId": "1ea9543b-cfe7-4f27-973b-0cf405217d51"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Neuron(\n",
              "  (fc): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neuron(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "H8KUxZS6vG8J",
        "outputId": "4cb9643c-a414-4caf-8765-68c2a01abfe9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3cacacc44adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-30a2acf33fb4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaviside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 14.76 GiB total capacity; 13.41 GiB already allocated; 557.75 MiB free; 13.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделано! Мы получили ошибку out of memory после того как все данные были, но еще не взаимодействовали с нейроном. Взаимодействие с ним также забирает себе GPU, причем внушительную часть (30% плюсом)"
      ],
      "metadata": {
        "id": "svazpXAcx-Rs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}